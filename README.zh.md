# 关于源码

独立研究者，业余时间研究，平时要上班，时间不充裕，以为论文会on hold一段时间，本打算待这几天时间准备代码仓库，没想到昨天提交arxiv，今天论文就正式发表了。

论文源码请稍等。

# TLinFormer 是如何诞生的？一段思考之旅
我们首先从一个普遍存在于稀疏化注意力机制中的问题开始：信息丢失。

在传统的稀疏化方法中，模型会计算全局 token 的注意力分数，然后无情地丢弃掉那些分数较低的 token。这种做法虽然高效，但代价是这些被丢弃的 token 所携带的信息可能就此永久丢失了，这对于需要长程依赖的任务来说可能是致命的。
## 第一步：一个“不丢弃任何信息”的稀疏化尝试
为了解决这个问题，我最初的版本选择了一条更温和的路线。我同样会选出注意力分数最高的 Top-K 个 "优等生" token，但我不会丢弃其他的 "普通" token。相反，我会让这些 "优等生" (Top-K tokens) 与所有的 token (全局信息) 再做一次交叉注意力，将全局的历史信息融合到自己身上。

这样一来，我们既有了稀疏化的计算效率，又没有粗暴地丢弃任何信息。
## 第二步：一个直击本质的疑问
但这时，一个关键问题浮现在我脑海里：这么做的本质是什么？

既然这些被选中的 "优等生" 最终总要回头去融合所有人的信息，那我们费尽心思挑选 "优等生" 的意义还大吗？如果神经网络足够强大，是不是我随机选择 Top-K 个 token，它也能通过参数优化，学会如何从全局信息中提取所需的一切？

这个想法让我意识到，注意力机制的本质，其实可以看作是在序列长度（L）这个维度上做了一次非常灵活、可学习的全连接。
## 第三步：从“局部性原理”中获得灵感
于是，我从计算机科学的一个经典概念——“局部性原理” (principle of locality) 中获得了灵感。程序在执行时，无论是时间上还是空间上，都倾向于访问邻近的数据。那么，对于 Transformer 来说，与当前输出窗口最邻近的那些 token，是不是天然就更重要呢？

所以，我放弃了全局搜索 "优等生" 的想法，而是直接选择了离当前输出窗口最近的 Top-K 个 token。这不仅在逻辑上更合理，计算上也更高效，并最终形成了 TLinFormer 如今独特的双窗口结构。
## 结论：一种更智能的稀疏化
所以，从这个角度看，TLinFormer 实际上也是一种稀疏化的解决方案。但它更高明，因为它把所有的优化工作完全交给了神经网络本身。

它不再需要我们去设计复杂的规则来 "猜" 哪些信息重要，也不再冒险丢弃任何信息。它只是提供了一个基于“局部性原理”的、简单高效的结构，然后让模型在训练中自己去学习如何最好地利用这些局部和全局的信息。